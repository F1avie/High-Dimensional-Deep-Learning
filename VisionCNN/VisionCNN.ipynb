{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Computer Vision\n",
    "_Example of the use of CNNs for object detection_\n",
    "\n",
    "---\n",
    "\n",
    "Object detection is a task in computer vision that involves identifying the presence, location, and type of one or more objects in a given photograph. It is a challenging problem that involves building upon methods for object recognition (_e.g._, where are they), object localization (_e.g._, what are their extent), and object classification (_e.g._, what are they).\n",
    "\n",
    "In recent years, deep learning techniques are achieving state-of-the-art results for object detection, such as on standard benchmark datasets and in computer vision competitions. Notable is the “You Only Look Once”, or YOLO, a family of Convolutional Neural Networks that achieve near state-of-the-art results with a single end-to-end model that can perform object detection in real-time.\n",
    "\n",
    "In this lab, we will first focus on the simpler problem of object localization. The localization problem considers that only one object is present on the image, while the detection problem tries to determine all the objects present on the image. Then, we will implement a simplified version of YOLO\n",
    "\n",
    "<!-- In this tutorial, we will put into practice some of the methods discussed in class to locate objects in an image. -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART I: Object Localization\n",
    "\n",
    "As said, in this section, we will focus on the simpler problem of locating a single object per image, _i.e._, locating the object associated with the dominant class of a classification algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen during the course, in both localization and detection, we try to simultaneously determine an object's position and class in the form of a bounding box. The bounding box is encoded by its width $b_w$, its height $b_h$, and its center whose coordinates are given by the pair $(b_x, b_y)$. \n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1_jHHv6ZDe-3Xz25jIZ6o177laBEfmMRR\" style=\"width:500;height:300px;\"></center>\n",
    "<caption><center><b>Figure 1:</b> Bounding box model used for localization</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "To begin, download the database images. You will find all the data at the following link: [Wildlife dataset](https://drive.google.com/drive/folders/16EYaEPi4zfq96XWEtkqW7m2gVdpXORu6?usp=sharing). _For the rest of the tutorial, we will need this data to be in a `wildlife` folder, in the current folder._\n",
    "\n",
    "The database has 4 classes, for the following 4 animals: buffalo, elephant, rhinoceros and zebra. It contains **376** images of each of the 4 classes.\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1sej2InBiQDEmpk2RA7S2dLRsJvF_UszE\" width=200>\n",
    "<img src=\"https://drive.google.com/uc?id=1K8cO4plzVIXO1MC4mkRcFIFjgspHtOIm\" width=200>\n",
    "<img src=\"https://drive.google.com/uc?id=15pkHpPW_VR1joOyPryS1pavOTqmoNR88\" width=200>\n",
    "<img src=\"https://drive.google.com/uc?id=19zHZn-A_j8Sx0G3V2SwR_YNKF0dGI1NA\" width=200></center>\n",
    "<caption><center><b>Figure 2:</b> Examples of images from the database</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Display an image of the training dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "# Display an image of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/disp_img.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** What is the format used in the database to encode the labels?</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will load the data and format it for classification. Some remarks about this function\n",
    "* To be able to use our images in a reasonable amount of time, we will have to start by resizing them.\n",
    "* We want the label to be of the form seen during the course, _i.e._, presence + bounding box + classes\n",
    "* If there are several objects in the same image, we will consider only the object whose bounding box takes up the largest area in the image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** We consider square images, denoted $x$, of size $s$ and we want to store them in a `numpy`array.</span>\n",
    "1. <span style=\"color:purple\">How big should the array for the image $x$ be?</span>\n",
    "\n",
    "Size for an image $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "<!-- $x$ being a square image of size $s$, in color (3 channels), it is of size $(s,s,3)$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <span style=\"color:purple\">We also want to store the labels, denoted $y$, in a `numpy` array. What size should this array be?</span>\n",
    "\n",
    "Size for a label $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "<!-- $y$ is a vector that must contain \"presence\" + \"bounding box\" + \"classes\". It is therefore size $9 = 1+4+4$. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# Total number of images\n",
    "DATASET_SIZE = ...  ### TO BE COMPLETED ###\n",
    "\n",
    "# Here we choose the dimension in which we will resize the images:\n",
    "# 64x64 is a rather small size but it will allow us to have faster experiments\n",
    "# 128x128 or 256x256 would give better results but at the cost of several hours of training\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_localization(image_size):\n",
    "    # Path to the database\n",
    "    ds_path = \"./wildlife/\"\n",
    "    # Paths to the data of the 4 different classes\n",
    "    paths = [ds_path + \"buffalo/\", ds_path + \"elephant/\", ds_path + \"rhino/\", ds_path + \"zebra/\"]\n",
    "    # Index for adding data to the x and y variables \n",
    "    i = 0\n",
    "    # Preparation of data structures for x and y\n",
    "    x = np.zeros((DATASET_SIZE, ...)) ### TO BE COMPLETED ### \n",
    "    y = np.empty((DATASET_SIZE, ...)) ### TO BE COMPLETED ### \n",
    "\n",
    "    # Browse paths of each class\n",
    "    for path in paths:\n",
    "\n",
    "        # Browse the (sorted) files in the directory\n",
    "        dirs = os.listdir(path)\n",
    "        dirs.sort()\n",
    "\n",
    "        for item in dirs:\n",
    "            if os.path.isfile(path + item):\n",
    "                # Extracting the file extension \n",
    "                extension =item.split(\".\")[1]\n",
    "\n",
    "            if extension == \"jpg\" or extension == \"JPG\":\n",
    "                # Image : we will fill the variable x\n",
    "                # Reading the image\n",
    "                img = Image.open(path + item)\n",
    "                # Image scaling\n",
    "                img = img.resize((image_size,image_size), Image.ANTIALIAS)\n",
    "                # Filling the variable x\n",
    "                x[i] = np.asarray(img)\n",
    "\n",
    "            elif extension == \"txt\":\n",
    "                # Text file: bounding box coordinates to fill y\n",
    "                labels = open(path + item, \"r\")\n",
    "                # Retrieving of lines from the text file\n",
    "                labels = labels.read().split('\\n')\n",
    "                # If the last line is empty, delete it \n",
    "                if labels[-1] == \"\":\n",
    "                    del labels[-1]\n",
    "\n",
    "                # Maximum area bounding box index\n",
    "                j_max = 0\n",
    "                if len(labels) > 1:\n",
    "                    area_max = 0 # Bounding box area of maximum area\n",
    "                    # Browse bounding boxes for objects in the image\n",
    "                    for j in range(len(labels)):\n",
    "                        # Compute the area of the current bounding box\n",
    "                        area = ... ### TO BE COMPLETED ###\n",
    "                        ### Notice that labels[j].split()[k] allows access to the k-th element of the label j ###\n",
    "                        # Update the maximum area bounding box, if necessary\n",
    "                        if area > area_max:\n",
    "                            area_max = area\n",
    "                            j_max = j    \n",
    "\n",
    "                # An object is present on the image (presence = 1)\n",
    "                presence = np.array([1], dtype=\"i\")\n",
    "                # \"One-hot vector \" to represent the class probabilities\n",
    "                classes = np_utils.to_categorical(...) ### TO BE COMPLETED ###\n",
    "                # Coordinates of the maximum area bounding box\n",
    "                coordinates = np.array(labels[j_max].split()[1:], dtype=\"f\")\n",
    "                # Filling the variable y\n",
    "                y[i, 0] = presence\n",
    "                y[i, 1:5] = coordinates\n",
    "                y[i, 5:] = classes\n",
    "\n",
    "                i = i + 1\n",
    "            else:\n",
    "                print(\"extension found: \", extension)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/load_data_localization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = load_data_localization(IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate our data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# To improve the training, we can center-reduce the coordinates of the bounding boxes\n",
    "y_std = np.std(y_train, axis=0)\n",
    "y_mean = np.mean(y_train, axis=0)\n",
    "y_train[:,1:5] = (y_train[:,1:5] - y_mean[1:5])/y_std[1:5]\n",
    "y_val[:,1:5] = (y_val[:,1:5] - y_mean[1:5])/y_std[1:5]\n",
    "\n",
    "# And normalize the color values\n",
    "x_train = x_train/255\n",
    "x_val = x_val/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxZ6cVouz9Lp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function allows to visualize the objects located on the images, _i.e._ on each image the selected bounding box, in a suitable color, and its associated class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJLRiuFX_VPL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If only x and y are indicated, an image number is randomly drawn and the y label associated with the image is displayed\n",
    "# If a 2nd y, named y_pred, is indicated, then the two labels are displayed side by side, so that they can be compared\n",
    "# Finally, you can also indicate the id of the image you wish to view.\n",
    "\n",
    "def print_data_localization(x, y, y_pred=[], id=None, image_size=IMAGE_SIZE):\n",
    "    if id==None:\n",
    "        # Random drawing of an image in the database\n",
    "        num_img = np.random.randint(x.shape[0]-1)\n",
    "    else:\n",
    "        num_img = id\n",
    "\n",
    "    img = x[num_img]\n",
    "    lab = y[num_img]\n",
    "\n",
    "    colors = [\"blue\", \"yellow\", \"red\", \"orange\"] # Different colors for different classes\n",
    "    classes = [\"Buffalo\", \"Elephant\", \"Rhino\", \"Zebra\"]\n",
    "\n",
    "    if np.any(y_pred):\n",
    "        plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Image display\n",
    "    ... ### TO BE COMPLETED ###\n",
    "    # Determination of the class\n",
    "    class_id = ... ### TO BE COMPLETED ###\n",
    "\n",
    "    # Determining the coordinates of the bounding box in the image frame\n",
    "    ax = (lab[1]*y_std[1] + y_mean[1]) * image_size\n",
    "    ay = (lab[2]*y_std[2] + y_mean[2]) * image_size\n",
    "    width = (lab[3]*y_std[3] + y_mean[3]) * image_size\n",
    "    height = (lab[4]*y_std[4] + y_mean[4]) * image_size\n",
    "    #print(\"x: {}, y: {}, w: {}, h:{}\".format(ax,ay,width, height))\n",
    "    # Determination of the extrema of the bounding box, namely the minimum and maximum x/y value for each box\n",
    "    p_x = [..., ...] ### TO BE COMPLETED ###\n",
    "    p_y = [..., ...] ### TO BE COMPLETED ###\n",
    "    # Display the bounding box in the right color\n",
    "    [...] ### TO BE COMPLETED ###\n",
    "    plt.title(\"Ground truth : Image {} - {}\".format(num_img, classes[class_id]))\n",
    "\n",
    "    if np.any(y_pred):\n",
    "        plt.subplot(1, 2, 2)\n",
    "        # Image display\n",
    "        ... ### TO BE COMPLETED ###\n",
    "        lab = y_pred[num_img]\n",
    "        # Determination of the class\n",
    "        class_id = ... ### TO BE COMPLETED ###\n",
    "\n",
    "        # Determining the coordinates of the bounding box in the image frame\n",
    "        ax = (lab[1]*y_std[1] + y_mean[1]) * image_size\n",
    "        ay = (lab[2]*y_std[2] + y_mean[2]) * image_size\n",
    "        width = (lab[3]*y_std[3] + y_mean[3]) * image_size\n",
    "        height = (lab[4]*y_std[4] + y_mean[4]) * image_size\n",
    "        #print(\"x: {}, y: {}, w: {}, h:{}\".format(ax,ay,width, height))\n",
    "        # Determination of the extrema of the bounding box\n",
    "        p_x = [..., ...] ### TO BE COMPLETED ###\n",
    "        p_y = [..., ...] ### TO BE COMPLETED ###\n",
    "        # Display the bounding box in the right color\n",
    "        [...] ### TO BE COMPLETED ###\n",
    "        plt.title(\"Prediction: Image {} - {}\".format(num_img, classes[class_id]))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/print_data_localization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(100): #x.shape[0]):\n",
    "    print_data_localization(x_train, y_train, image_size=IMAGE_SIZE, id=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4wQYq3AKnA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Useful functions\n",
    "\n",
    "1 . Computation of the **IoU** coefficient (Intersection over Union) between real and predicted bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMntCEgkANMg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVk9cB1WAMUK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_iou(y_true, y_pred):\n",
    "    ### \"Denormalization\" of bounding box coordinates\n",
    "    pred_box_xy = y_pred[:, ...] * y_std[...] + y_mean[...] ### TO BE COMPLETED ###\n",
    "    true_box_xy = y_true[:, ...] * y_std[...] + y_mean[...] ### TO BE COMPLETED ###\n",
    "\n",
    "    ### \"Denormalization of the width and height of bounding boxes\n",
    "    pred_box_wh = y_pred[:, ...] * y_std[...] + y_mean[...] ### TO BE COMPLETED ###\n",
    "    true_box_wh = y_true[:, ...] * y_std[...] + y_mean[...] ### TO BE COMPLETED ###\n",
    "\n",
    "    # Computation of the minimum and maximum coordinates of the real bounding box\n",
    "    true_mins   = ... ### TO BE COMPLETED ###\n",
    "    true_maxs   = ... ### TO BE COMPLETED ###\n",
    "\n",
    "    # Computation of the minimum and maximum coordinates of the predicted bounding box\n",
    "    pred_mins   = ... ### TO BE COMPLETED ###\n",
    "    pred_maxs   = ... ### TO BE COMPLETED ###    \n",
    "\n",
    "    # Determining the intersection of bounding boxes\n",
    "    intersect_mins  = tf.maximum(pred_mins, true_mins)\n",
    "    intersect_maxs  = tf.minimum(pred_maxs, true_maxs)\n",
    "    intersect_wh    = tf.maximum(intersect_maxs - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[:, 0] * intersect_wh[:, 1]\n",
    "\n",
    "    # Area of predicted and actual bounding boxes\n",
    "    true_areas = ... ### TO BE COMPLETED ###\n",
    "    pred_areas = ... ### TO BE COMPLETED ###\n",
    "\n",
    "    # Area of the union of predicted and real boxes\n",
    "    union_areas = ... ### TO BE COMPLETED ###\n",
    "\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "    return iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/compute_iou.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou():\n",
    "    def iou_metrics(y_true, y_pred):\n",
    "        return compute_iou(y_true, y_pred)\n",
    "    iou_metrics.__name__= \"IoU\"\n",
    "    return iou_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualization of learning quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6O7R3qnCtlN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training_analysis(history, metric='loss'):    \n",
    "\n",
    "    loss = history.history[metric]\n",
    "    val_loss = history.history['val_' + metric]\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', linestyle=\"--\",label='Training ' + metric)\n",
    "    plt.plot(epochs, val_loss, 'g', label='Validation ' + metric)\n",
    "    plt.title('Training and validation ' + metric)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F_l_yNlDapa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A first example of CNN for object localization\n",
    "\n",
    "This part aims to develop a first example of a convolutional network for object localization.\n",
    "\n",
    "To do this, complete the codes provided to obtain a localization algorithm. You can use any convolutional base of your choice, but you must pay special attention to the output layer. Actually, you will produce three different outputs:\n",
    "* one characterizing the presence of an object, \n",
    "* another providing the coordinates of the bounding box, and finally, \n",
    "* a last one performing the classification.\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=14GsrBEwbb_zllj6UAJv43dy9BUkq1IG0\" width=500></center>\n",
    "<caption><center><b>Figure 3:</b> Illustration of the network architecture to be built.</center></caption>\n",
    "\n",
    "<!-- <center> <img src=\"https://drive.google.com/uc?id=1bnh8zU7Os-w-5TT8hV4xDoThKQc-Ywc2\" width=500></center>\n",
    "<caption><center> Figure 4: Illustration des fonctions de coût à utiliser pour l'entraînement. </center></caption> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6D31NGPNyJp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Input\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPOXiJ7hDcbr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_localization(input_shape=(64, 64, 3)):\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    ### TO BE COMPLETED ###\n",
    "    conv1 = Conv2D(...)(input_layer)\n",
    "    # Convolutions, Pooling, Dense,...  Up to you to form your network!\n",
    "    # Your last layer must update a variable x, reused in the output layers below\n",
    "    x = ...\n",
    "\n",
    "    output_p     = Dense(..., activation=..., name='p')(x)       # Output characterizing the presence of an object\n",
    "    output_coord = Dense(..., activation=..., name='coord')(x)   # Output characterizing bounding box coordinates\n",
    "    output_class = Dense(..., activation=..., name='classes')(x) # Output characterizing the class probabilities\n",
    "\n",
    "    output = [output_p, output_coord, output_class]\n",
    "    model  = Model(input_layer, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/create_model_localization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYoVYfQpotjy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You must associate a cost function with each network output to train your network. The total cost function will be the sum of the three previously defined cost functions, weighted by weights defined in the variable `loss_weights`.\n",
    "\n",
    "**Take the time to test different values of** `loss_weights` **depending on the evolution of the metrics you observe during the training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_ewlCn5Rovm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "model = create_model_localization()\n",
    "opt   = Adam(learning_rate=3e-4)  \n",
    "\n",
    "### TO BE COMPLETED ###\n",
    "# Here put, in order, the cost functions associated with each output\n",
    "loss=[..., ..., ...]\n",
    "\n",
    "# We will associate a metric to each output: the accuracy for the two classifications,  \n",
    "# and the IoU defined earlier for the quality of the bounding boxes. \n",
    "metrics=[ ['accuracy'], [iou()], ['accuracy']]\n",
    "loss_weights = [1, 1, 1] ### Vary these WEIGHTS to find an ad hoc COMBINATION ###\n",
    "\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=metrics,\n",
    "              loss_weights=loss_weights\n",
    "              )\n",
    "\n",
    "history = model.fit(x_train, [y_train[:,0], y_train[:,1:5], y_train[:,5:9]],\n",
    "              epochs=30,\n",
    "              batch_size=batch_size,            \n",
    "              validation_data=(x_val, [y_val[:,0], y_val[:,1:5], y_val[:,5:9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/train_model_localization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical validation\n",
    "\n",
    "1. _Analysis of the results:_ Curves of the evolution of the loss function and of the IoU of the bounding boxes, as well as of the accuracy of the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc-sGe8LNzA-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_training_analysis(history, metric='loss')\n",
    "plot_training_analysis(history, metric='coord_IoU')\n",
    "plot_training_analysis(history, metric='classes_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prediction of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_presence, y_pred_coords, y_pred_classes = model.predict(x_val)\n",
    "y_pred = np.zeros(y_val.shape)\n",
    "\n",
    "for i in range(y_pred.shape[0]):\n",
    "    y_pred[i, 0] = y_pred_presence[i]\n",
    "    y_pred[i, 1:5] = y_pred_coords[i]\n",
    "    y_pred[i, 5:9] = y_pred_classes[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Display of results on several images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=2, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=3, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=7, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=25, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=16, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=18, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=24, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=15, image_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiFO6fBns-OI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Improvement of the cost function\n",
    "\n",
    "In practice, it is tricky to find a good combination of loss functions as you did on the previous cells. The cross-entropy and the mean square error give values that are too different from being combined effectively.\n",
    "\n",
    "A variant, perhaps simpler to do work, is to use only the mean square error as the loss for all outputs. This variant is implemented in the YOLO algorithm, of which we will implement a variant in the second part of this tutorial. Test this solution below. As in the previous exercise, feel free to vary the weights of the different elements of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttGCeqz0Dc3y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "model = create_model_localization()\n",
    "opt = Adam(learning_rate=3e-4)  \n",
    "\n",
    "loss = [..., ..., ...]\n",
    "metrics =[ ['accuracy'], [iou()], ['accuracy']]\n",
    "loss_weights = [...]\n",
    "\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=metrics,\n",
    "              loss_weights=loss_weights\n",
    "              )\n",
    "\n",
    "history = model.fit(x_train, [y_train[:,0], y_train[:,1:5], y_train[:,5:9]],\n",
    "              epochs=30,\n",
    "              batch_size=batch_size,            \n",
    "              validation_data=(x_val, [y_val[:,0], y_val[:,1:5], y_val[:,5:9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/train_model_localization_MSE.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical validation\n",
    "\n",
    "1. _Analysis of the results:_ Curves of the evolution of the loss function and of the IoU of the bounding boxes, as well as of the accuracy of the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc-sGe8LNzA-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_training_analysis(history, metric='loss')\n",
    "plot_training_analysis(history, metric='coord_IoU')\n",
    "plot_training_analysis(history, metric='classes_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prediction of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_presence, y_pred_coords, y_pred_classes = model.predict(x_val)\n",
    "y_pred = np.zeros(y_val.shape)\n",
    "\n",
    "for i in range(y_pred.shape[0]):\n",
    "    y_pred[i, 0] = y_pred_presence[i]\n",
    "    y_pred[i, 1:5] = y_pred_coords[i]\n",
    "    y_pred[i, 5:9] = y_pred_classes[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Display of results on several images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=2, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=3, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=7, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=25, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=16, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=18, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=24, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=15, image_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stQpmnmAt_bf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Some axes of improvement\n",
    "\n",
    "Considering the small size of the database, the results are not bad! There is some confusion between some classes, but the predictions are often interesting.\n",
    "\n",
    "However, there should still be strong overfitting at this stage. There are several possibilities to correct it: \n",
    " \n",
    "* _Regularization_ by weight decay, using `kernel_regularizer` on the network layers\n",
    "* _Data augmentation_: For example, with a `Sequence`class and the use of the `Albumentation` library.\n",
    "* Use of _transfer learning_: Starting from a network trained on *ImageNet* (which contains many classes of animals), we would benefit from very general filters which would help to limit overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False, # The Dense part of the original network is not kept\n",
    "                  input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_localization(input_shape=(64, 64, 3)):\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    vgg = conv_base(input_layer)\n",
    "    x = Flatten()(vgg)\n",
    "\n",
    "    output_p = Dense(1, activation='sigmoid', name='p')(x)\n",
    "    output_coord = Dense(4, activation='linear', name='coord')(x)\n",
    "    output_class = Dense(4, activation='softmax', name='classes')(x)\n",
    "\n",
    "    output= [output_p, output_coord, output_class]\n",
    "    model = Model(input_layer, output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model_localization()\n",
    "conv_base.trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "model = create_model_localization()\n",
    "opt = Adam(learning_rate=3e-4)  \n",
    "\n",
    "loss=['binary_crossentropy', 'mse', 'categorical_crossentropy']\n",
    "metrics=[ ['accuracy'], [iou()], ['accuracy']]\n",
    "loss_weights = [1, 5, 1]\n",
    "\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=metrics,\n",
    "              loss_weights=loss_weights\n",
    "              )\n",
    "\n",
    "history = model.fit(x_train, [y_train[:,0], y_train[:,1:5], y_train[:,5:9]],\n",
    "              epochs=30,\n",
    "              batch_size=batch_size,            \n",
    "              validation_data=(x_val, [y_val[:,0], y_val[:,1:5], y_val[:,5:9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. _Analysis of the results:_ Curves of the evolution of the loss function and of the IoU of the bounding boxes, as well as of the accuracy of the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc-sGe8LNzA-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_training_analysis(history, metric='loss')\n",
    "plot_training_analysis(history, metric='coord_IoU')\n",
    "plot_training_analysis(history, metric='classes_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prediction of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_presence, y_pred_coords, y_pred_classes = model.predict(x_val)\n",
    "y_pred = np.zeros(y_val.shape)\n",
    "\n",
    "for i in range(y_pred.shape[0]):\n",
    "    y_pred[i, 0] = y_pred_presence[i]\n",
    "    y_pred[i, 1:5] = y_pred_coords[i]\n",
    "    y_pred[i, 5:9] = y_pred_classes[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Display of results on several images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=2, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=3, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=7, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=25, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=16, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=18, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=24, image_size=IMAGE_SIZE)\n",
    "print_data_localization(x_val, y_val, y_pred = y_pred, id=15, image_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART II: Implementation of a simplified version of YOLO\n",
    "\n",
    "In this part, we will try to go further by considering the more complex problem of object detection, _i.e._, the joint localization and classification of all the objects in the image; for that, we will implement a simplified version of YOLO.\n",
    "\n",
    "The simplification comes from the fact that we will not include all the elements described in [Redmon](https://pjreddie.com/)'s article (for example, the optimizer's choice). One of the main simplifications is that _we will only consider one object per cell_.\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1V4aAS7K_Akj83apuMZ2vRjNvjgdgoOCh\" width=500></center>\n",
    "<caption><center><b>Figure 4</b>: Pipeline of the YOLO algorithm <a href=\"https://pjreddie.com/media/files/papers/yolo_1.pdf\">[Redmon 2016]</a></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the idea of YOLO is to slice the image into a grid of cells and perform a prediction of several bounding boxes as well as a classification per cell.\n",
    "\n",
    "We use here the same dataset : the [Wildlife](https://drive.google.com/drive/folders/16EYaEPi4zfq96XWEtkqW7m2gVdpXORu6?usp=sharing) database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions\n",
    "\n",
    "Definition of the different variables useful for the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64  # Size of the input images of the network\n",
    "CELL_PER_DIM = 8 # Number of cells in width and height\n",
    "BOX_PER_CELL = 1 # Number of objects per cell\n",
    "NB_CLASSES = 4   # Number of classes of the problem\n",
    "PIX_PER_CELL = IMAGE_SIZE/CELL_PER_DIM\n",
    "\n",
    "DATASET_SIZE = 376*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** What is the size of the $y$-array in the YOLO framework?</span>\n",
    "\n",
    "Size for a label $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "<!-- $y$ is a vector of size $(S,S,N+5k)$, where:\n",
    "* $S$ is the grid size, or with the notations introduced above `CELL_PER_DIM`,\n",
    "* $N$ is the number of classes, _i.e._, `NB_CLASSES`, and\n",
    "* $k$ is the number of objects detected per cell _i.e._ `BOX_PER_CELL`.\n",
    "\n",
    "In other words, $y$ is of size (`CELL_PER_DIM`,`CELL_PER_DIM`,`NB_CLASSES`+5*`BOX_PER_CELL`). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and formatting for the detection problem. \n",
    "\n",
    "The function defined hereafter is the pendant of the `load_data_detection` function previously defined.\n",
    "\n",
    "Problem data (more than one bounding box per cell) are indicated and displayed during loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVz03XXMKMoz"
   },
   "outputs": [],
   "source": [
    "def load_data_detection():\n",
    "    # Path to the database\n",
    "    ds_path = \"./wildlife/\"\n",
    "    # Paths to the data of the 4 different classes\n",
    "    paths = [ds_path+\"buffalo/\", ds_path+\"elephant/\", ds_path+\"rhino/\", ds_path+\"zebra/\"]\n",
    "    # Index for adding data to the x and y variables \n",
    "    i = 0\n",
    "    # Preparation of data structures for x and y\n",
    "    x = np.zeros((DATASET_SIZE, ...)) ### TO BE COMPLETED ### \n",
    "    y = np.zeros((DATASET_SIZE, ...)) ### TO BE COMPLETED ### \n",
    "\n",
    "    # Save normalized bounding box width/height\n",
    "    widths = []\n",
    "    heights = []\n",
    "\n",
    "    # Browse paths of each class\n",
    "    for path in paths:\n",
    "\n",
    "        # Browse the (sorted) files in the directory\n",
    "        dirs = os.listdir(path)\n",
    "        dirs.sort()\n",
    "\n",
    "        for item in dirs:\n",
    "            if os.path.isfile(path + item):\n",
    "                # Extracting the file extension \n",
    "                extension = item.split(\".\")[1]\n",
    "\n",
    "                if extension==\"jpg\" or extension==\"JPG\":\n",
    "                    # Image : we will fill the variable x\n",
    "                    # Reading the image\n",
    "                    img = Image.open(path + item)\n",
    "                    # Image scaling\n",
    "                    img = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n",
    "                    # Filling the variable x\n",
    "                    x[i] = np.asarray(img, dtype=np.int32)\n",
    "\n",
    "                elif extension==\"txt\":\n",
    "                    # Text file: bounding box coordinates to fill y\n",
    "                    labels = open(path + item, \"r\")\n",
    "                    # Retrieving of lines from the text file\n",
    "                    labels = labels.read().split('\\n')\n",
    "                    # If the last line is empty, delete it \n",
    "                    if labels[-1]==\"\":\n",
    "                        del labels[-1]\n",
    "\n",
    "                    err_flag = 0\n",
    "                    boxes = []\n",
    "                    for label in labels:\n",
    "                        # Retrieving information from the bounding box\n",
    "                        label = label.split()\n",
    "                        # Save width/height of bounding boxes\n",
    "                        widths.append(float(...))  ### TO BE COMPLETED ### \n",
    "                        heights.append(float(...)) ### TO BE COMPLETED ### \n",
    "                        # Bounding box center coordinates in the image frame\n",
    "                        cx, cy = float(...) * IMAGE_SIZE, float(...) * IMAGE_SIZE ### TO BE COMPLETED (x2) ### \n",
    "                        # Determination of the indices of the cell in which the center falls\n",
    "                        ind_x, ind_y = int(cx // PIX_PER_CELL), int(cy // PIX_PER_CELL)\n",
    "                        # YOLO : \"The (x, y) coordinates represent the center of the box relative to the bounds of the grid cell.\"\n",
    "                        # -> Compute the coordinates of the center relative to the cell in which it is located\n",
    "                        cx_cell = (cx - ind_x * PIX_PER_CELL) / PIX_PER_CELL\n",
    "                        cy_cell = (cy - ind_y * PIX_PER_CELL) / PIX_PER_CELL\n",
    "                        # Confidence index of the bounding box\n",
    "                        presence = np.array(...) ### TO BE COMPLETED ### \n",
    "                        # \"One-hot vector \" to represent the class probabilities\n",
    "                        classes = ... ### TO BE COMPLETED ### \n",
    "                        # We arrange the class probabilities at the end of the vector ([ BOX 1 ; BOX 2 ; ... ; BOX N ; CLASSES])\n",
    "                        y[i, ind_x, ind_y, 5 * BOX_PER_CELL:] = classes\n",
    "\n",
    "                        boxes.append([cx, cy, label[3]*IMAGE_SIZE, label[4]*IMAGE_SIZE])\n",
    "                        # Determining the cell bounding box index in which to store the information\n",
    "                        ind_box = 0\n",
    "                        while y[i, ind_x, ind_y, 5*ind_box] == 1 and ind_box < BOX_PER_CELL - 1:\n",
    "                            # If the current index box is already in use (presence = 1)  \n",
    "                            # and the maximum number of boxes has not been reached, we go to the next box\n",
    "                            ind_box = ind_box + 1\n",
    "\n",
    "                        if y[i, ind_x, ind_y, 5*ind_box] == 1:\n",
    "                            print(\"ERROR: THE CELL ALREADY CONTAINS ALL THE AVAILABLE BOXES\")\n",
    "                            print(path + item)\n",
    "                            err_flag = 1\n",
    "                        else:\n",
    "                            y[i, ind_x, ind_y, 5*ind_box] = 1\n",
    "                            y[i, ind_x, ind_y, 5*ind_box + 1] = cx_cell\n",
    "                            y[i, ind_x, ind_y, 5*ind_box + 2] = cy_cell\n",
    "                            # Square root of the width and height of the box\n",
    "                            y[i, ind_x, ind_y, 5*ind_box + 3] = math.sqrt(float(...)) ### TO BE COMPLETED : width ### \n",
    "                            y[i, ind_x, ind_y, 5*ind_box + 4] = math.sqrt(float(...)) ### TO BE COMPLETED : height ### \n",
    "\n",
    "                    i = i + 1\n",
    "                    if err_flag == 1:\n",
    "                        img_name = item.split(\".\")[0]\n",
    "                        img = Image.open(path + img_name + '.jpg')\n",
    "                        # Image scaling\n",
    "                        img = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n",
    "\n",
    "                        plt.imshow(img)\n",
    "                        for ind_cell in range(CELL_PER_DIM):                \n",
    "                            plt.plot([ind_cell*PIX_PER_CELL, ind_cell*PIX_PER_CELL], [0, IMAGE_SIZE-1], 'k-')\n",
    "                            plt.plot([0, IMAGE_SIZE-1], [ind_cell*PIX_PER_CELL, ind_cell*PIX_PER_CELL], 'k-')\n",
    "\n",
    "                        for ind_box_plot in range(len(boxes)):\n",
    "                            box = boxes[ind_box_plot]\n",
    "                            plt.plot(box[0], box[1], 'b.')\n",
    "                        plt.show()\n",
    "\n",
    "                else:\n",
    "                    print(\"extension found: \", extension)\n",
    "\n",
    "    return x, y, widths, heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/load_data_detection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = load_data_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the database is divided between training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Image normalization\n",
    "x_train = x_train/255\n",
    "x_val = x_val/255\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display of data and detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_detection(x, y, id=None, image_size=IMAGE_SIZE, mode='gt'):\n",
    "    if id==None:\n",
    "        # Random drawing of an image in the database\n",
    "        num_img = np.random.randint(x.shape[0]) \n",
    "        print(num_img)\n",
    "    else:\n",
    "        num_img = id\n",
    "\n",
    "    img = x[num_img]\n",
    "    lab = y[num_img]\n",
    "\n",
    "    colors = [\"blue\", \"yellow\", \"red\", \"orange\"] # Different colors for the different classes\n",
    "    classes = [\"Buffalo\", \"Elephant\", \"Rhino\", \"Zebra\"]\n",
    "\n",
    "    boxes = lab[:, :, 1:5]\n",
    "    for ind_x in range(CELL_PER_DIM):\n",
    "        for ind_y in range(CELL_PER_DIM):\n",
    "            box = boxes[ind_x, ind_y]\n",
    "            box[0] = ... ### TO BE COMPLETED ### \n",
    "            box[1] = ... ### TO BE COMPLETED ### \n",
    "            box[2] = box[2]**2 * IMAGE_SIZE\n",
    "            box[3] = box[3]**2 * IMAGE_SIZE\n",
    "            boxes[ind_x, ind_y] = box\n",
    "\n",
    "    # Retrieve all information from bounding boxes\n",
    "    all_presences = np.reshape(lab[:, :, 0], (CELL_PER_DIM*CELL_PER_DIM))\n",
    "    all_boxes = np.reshape(lab[:, :, 1:5], (-1, 4))\n",
    "    all_classes = np.reshape(lab[:, :, 5:9], (-1, 4))\n",
    "\n",
    "    if mode=='pred':\n",
    "        all_presences = 1 / (1 + np.exp(-all_presences))\n",
    "        all_classes = softmax(all_classes, axis=1)\n",
    "\n",
    "    indices_sorted = np.argsort(-all_presences)\n",
    "\n",
    "    # Eliminate all bounding boxes whose probability of presence is < threshold \n",
    "    threshold = 0.35\n",
    "    all_boxes = ... ### TO BE COMPLETED ### \n",
    "    all_classes = ... ### TO BE COMPLETED ### \n",
    "    all_presences = ... ### TO BE COMPLETED ### \n",
    "\n",
    "\n",
    "    # Image display\n",
    "    plt.imshow(img)\n",
    "    for i in range(all_boxes.shape[0]):\n",
    "\n",
    "        # Determination of the class\n",
    "        class_id = ... ### TO BE COMPLETED ###\n",
    "        lab = all_boxes[i]\n",
    "        # Determination of the extrema of the bounding box\n",
    "        p_x = [..., ...] ### TO BE COMPLETED ###\n",
    "        p_y = [..., ...] ### TO BE COMPLETED ###\n",
    "        # Display the bounding box in the right color\n",
    "        [...] ### TO BE COMPLETED ###\n",
    "  \n",
    "    plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/print_data_detection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_detection(x_train, y_train, image_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simplified version of YOLO\n",
    "\n",
    "The model proposed below is only one possibility among many others. The Redmon article mentions a delicate instability during the training. So, we chose to use an elu (exponential linear unit) activation function.\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=17_-sM2a1rMxvdkhd8FiQCX4pSt0EBuNC\" width=500></center>\n",
    "<caption><center><b>Figure 5</b>: YOLO output layer</a></center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the last layer to have the right size output.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_YOLO(input_shape=(64, 64, 3)):\n",
    "    weight_decay = 0\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    conv1 = Conv2D(32, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(input_layer)\n",
    "    conv1 = Conv2D(32, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = Conv2D(32, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(64, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = Conv2D(64, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(128, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = Conv2D(128, 3, activation = 'elu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    dense4 = Flatten()(pool3)\n",
    "    dense4 = Dense(512, activation='elu',kernel_regularizer=regularizers.l2(weight_decay))(dense4)\n",
    "    dense5 = Dense(512, activation='elu',kernel_regularizer=regularizers.l2(weight_decay))(dense4)\n",
    "    output = Dense(..., activation='linear',kernel_regularizer=regularizers.l2(weight_decay))(dense5) ### TO BE COMPLETED ###\n",
    "    output = Reshape((...))(output) ### TO BE COMPLETED ###\n",
    "\n",
    "    model = Model(input_layer, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/create_model_YOLO.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_YOLO()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "We now come to the tricky part of the YOLO implementation: the definition of the cost function to use.\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1CM4EJks6Tz-6uBK6EwUmQkuhe8ef6o5A\" style=\"width:500;height:300px;\"></center>\n",
    "<caption><center> Detail of the loss function defined in the YOLO v1 article</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing a custom cost function in `keras`, it is necessary to use only the functions on the following page: https://keras.rstudio.com/articles/backend.html. \n",
    "Indeed this cost function, which will be called during the training, will treat tensors, not `numpy` arrays. We must therefore use the `tensorflow` library, which allows us to manipulate tensors.\n",
    "\n",
    "An essential part of the function is already written: the one that allows the separation of the data of the so-called \"empty\" cells (the ground truth does not contain a bounding box) from the \"non-empty\" ones.\n",
    "\n",
    "The details of the cost function are shown above: in the article $\\lambda_{\\text{coord}} = 5$ and $\\lambda_{\\text{noobj}} = 0.5$. \n",
    "* The $x_i$, $y_i$, $w_i$, and $h_i$ correspond to the coordinates of a bounding box; \n",
    "* $C_i$ corresponds to the probability of the presence of an object in the cell (sigmoid function applied to the corresponding output elements); and \n",
    "* the $p_i(c)$ are the class probabilities (softmax function applied to the corresponding output elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Reshape, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Complete the following function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the YOLO loss function\n",
    "def YOLOss(lambda_coord, lambda_noobj, batch_size):\n",
    "\n",
    "    # \"Green\" part: subpart concerning the confidence index \n",
    "    # and the class probabilities probabilities in the case where a box is present in the cell\n",
    "    def box_loss(y_true, y_pred):\n",
    "        return ... ### TO BE COMPLETED ###\n",
    "\n",
    "    # \"Blue\" part: subpart concerning the coordinates of the bounding box in the case where a box is present in the cell\n",
    "    def coord_loss(y_true, y_pred):\n",
    "        return ... ### TO BE COMPLETED ###\n",
    "\n",
    "\n",
    "    # \"Red\" part: subpart concerning the confidence index in case no box is present in the cell\n",
    "    def nobox_loss(y_true, y_pred):\n",
    "        return ... ### TO BE COMPLETED ###\n",
    "\n",
    "\n",
    "    def YOLO_loss(y_true, y_pred):\n",
    "\n",
    "        # Reshape the tensors from bs x S x S x (5B+C) to (bsxSxS) x (5B+C)\n",
    "        y_true = K.reshape(y_true, shape=(-1, 9))\n",
    "        y_pred = K.reshape(y_pred, shape=(-1, 9))\n",
    "\n",
    "        # Search (in y_true labels) for indices of cells for which at least the first bounding box is present\n",
    "        not_empty = K.greater_equal(y_true[:, 0], 1)      \n",
    "        indices = K.arange(0, K.shape(y_true)[0])\n",
    "        indices_notempty_cells = indices[not_empty]\n",
    "\n",
    "        empty = K.less_equal(y_true[:, 0], 0)\n",
    "        indices_empty_cells = indices[empty]\n",
    "\n",
    "        # Separate the cells of y_true and y_pred with or without bounding box\n",
    "        y_true_notempty = K.gather(y_true, indices_notempty_cells)\n",
    "        y_pred_notempty = K.gather(y_pred, indices_notempty_cells)\n",
    "\n",
    "        y_true_empty = K.gather(y_true, indices_empty_cells)\n",
    "        y_pred_empty = K.gather(y_pred, indices_empty_cells)\n",
    "\n",
    "        return (box_loss(y_true_notempty, y_pred_notempty) + lambda_coord*coord_loss(y_true_notempty, y_pred_notempty) + lambda_noobj*nobox_loss(y_true_empty, y_pred_empty))/batch_size\n",
    "\n",
    "   \n",
    "    # Return a function\n",
    "    return YOLO_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/YOLOss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size=16\n",
    "model = create_model_YOLO()\n",
    "opt = Adam(learning_rate=1e-4)  \n",
    "\n",
    "\n",
    "# As the training is unstable, we trigger a model backup each time the validation loss reaches a new minimum\n",
    "model_saver = ModelCheckpoint('tmp/best_weights', monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "loss=[YOLOss(5, 0.5, batch_size)]\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=opt)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "              epochs=100,\n",
    "              batch_size=batch_size,           \n",
    "              validation_data=(x_val, y_val),\n",
    "              callbacks = [model_saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Test of the version at the end of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "print_data_detection(x_train, y_pred, image_size=IMAGE_SIZE, mode='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "print_data_detection(x_val, y_pred, image_size=IMAGE_SIZE, mode='pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqMHwVaJ1iyN"
   },
   "source": [
    "2. Test the _best_ saved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRjn1SLn0JL4",
    "outputId": "2b797a0c-fa6a-49bc-ecd4-679d777971d0"
   },
   "outputs": [],
   "source": [
    "model.load_weights('tmp/best_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "dfytX2yC1AdY",
    "outputId": "3ac092ad-27aa-4b8d-8188-2654f5486f8f"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "print_data_detection(x_train, y_pred, image_size=IMAGE_SIZE, mode='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "zBiTzmyu0QqH",
    "outputId": "9e4282a8-3d52-463d-cb19-6166f0fa06d1"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "print_data_detection(x_val, y_pred, image_size=IMAGE_SIZE, mode='pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAlNpxA3uxBr"
   },
   "source": [
    "### Loading weights from an already trained network\n",
    "\n",
    "As YOLO training is very unstable, it is possible that at the end of this tutorial you will not get very convincing results. To finish this tutorial, you will find below the weights of a model trained for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Flhlq1874H6F",
    "outputId": "0460acab-c10c-4558-ecf2-c39380f19f81"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1q9PW6VwiVhS2MgRHdN9BlxXZxm2dgnTv' -O best_weights.index\n",
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1_mtwH4W4vRf-5yGRFjHZHiGzJDr9cYBb' -O best_weights.data-00000-of-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9SXVXanSmpe",
    "outputId": "adfd417e-97ea-419a-a73e-d051d3f59475"
   },
   "outputs": [],
   "source": [
    "model.load_weights('best_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "RCNoAH9grggS",
    "outputId": "30fe951a-2618-4ae7-b5b8-ba431c47642d"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "print_data_detection(x_train, y_pred, image_size=IMAGE_SIZE, mode='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "qjGN0rbawQVK",
    "outputId": "91d845a8-c3db-4569-aa2a-58f992c8af53"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "print_data_detection(x_val, y_pred, image_size=IMAGE_SIZE, mode='pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QQpj1Me7W8K"
   },
   "source": [
    "The results are not perfect, but we are starting to see some good results. As previously, we could limit  overfitting by using data augmentation in this training. \n",
    "\n",
    "We can already see in the few examples below that some of the images are rather well-predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "33ox7P-e4SJ2",
    "outputId": "ec77327f-9141-49d3-c722-dede71e9e441"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "\n",
    "print_data_detection(x_val, y_pred, id=81,  image_size=IMAGE_SIZE, mode='pred')\n",
    "print_data_detection(x_val, y_pred, id=28,  image_size=IMAGE_SIZE, mode='pred')\n",
    "print_data_detection(x_val, y_pred, id=37,  image_size=IMAGE_SIZE, mode='pred')\n",
    "print_data_detection(x_val, y_pred, id=220, image_size=IMAGE_SIZE, mode='pred')\n",
    "print_data_detection(x_val, y_pred, id=214, image_size=IMAGE_SIZE, mode='pred')\n",
    "print_data_detection(x_val, y_pred, id=193, image_size=IMAGE_SIZE, mode='pred')\n",
    "print_data_detection(x_val, y_pred, id=39,  image_size=IMAGE_SIZE, mode='pred')\n",
    "print_data_detection(x_val, y_pred, id=108, image_size=IMAGE_SIZE, mode='pred')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP9Ho8XgYf4uc8r8fEoDKzU",
   "collapsed_sections": [],
   "name": "IAM2020 - TP5 - Détection d'objet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
